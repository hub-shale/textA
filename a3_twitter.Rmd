---
title: "Twitter Sentiment"
author: "Shale"
date: "4/20/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(quanteda)
#devtools::install_github("quanteda/quanteda.sentiment") #not available currently through CRAN
library(quanteda.sentiment)
library(quanteda.textstats)
library(tidyverse)
library(tidytext)
library(lubridate)
library(wordcloud) #visualization of common words in the data set
library(reshape2)
library(DT)
```

# Loading & Further Cleaning

```{r}
raw_tweets <- read.csv("https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/dat/IPCC_tweets_April1-10_sample.csv", header=TRUE)

dat <- raw_tweets[,c(5,7)] # Extract Date and Title fields

tweets <- tibble(text = dat$Title,
                  id = seq(1:length(dat$Title)),
                 date = as.Date(dat$Date,'%m/%d/%y'))
```

A quick look:

```{r}
head(tweets$text, n = 10)
#simple plot of tweets per day

tweets %>%
  count(date) %>%
  ggplot(aes(x = date, y = n))+
  geom_line()
```

```{r}
#let's clean up the URLs from the tweets
clean_twt = tweets

clean_twt$text <- gsub("http[^[:space:]]*", "",clean_twt$text)
clean_twt$text <- gsub("@[^[:space:]]*", "", clean_twt$text)
clean_twt$text <- str_to_lower(clean_twt$text)

```


# Most Common Words by Day

```{r}
pop = clean_twt %>% 
  unnest_tokens(output = word, input = text, token = "words") %>%
  anti_join(stop_words, by = "word") %>% 
  group_by(date, word) %>% 
  summarise(daily_count = n()) %>% 
  slice_max(daily_count, n=10, with_ties = FALSE)
  

ggplot(data = pop, aes(x=daily_count, y=reorder(word, daily_count))) + 
  geom_bar(stat = "identity") +
  facet_wrap(facets = "date", scales = "free")
```

The words `ipcc`, `climate`, and `report` are consistently the most frequently used words per day, which makes sense in a query for "IPCC." There is a distinct pattern in the count of the most common words each day: on April 1-2 there are very few mentions, probably an indicator of fewer tweets overall that fit the query; April 3rd the numbers get higher, with the term `ipcc` showing up about 100 times. Count of these words max out on April 4-5, with many hundreds of uses of `ipcc`, `climate`, and `report`; then usage steadily declines again from April 6-10.

# Wordcloud

```{r}
#load sentiment lexicons
bing_sent <- get_sentiments('bing')
nrc_sent <- get_sentiments('nrc')

#tokenize tweets to individual words
words <- clean_twt %>%
  select(id, date, text) %>%
  unnest_tokens(output = word, input = text, token = "words") %>%
  anti_join(stop_words, by = "word") %>%
  left_join(bing_sent, by = "word") %>%
  left_join(
    tribble(
      ~sentiment, ~sent_score,
      "positive", 1,
      "negative", -1),
    by = "sentiment")
```


```{r}
words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("red", "green"),
                   max.words = 100)


```

# Most Tagged Accounts

```{r}
# using quanteda and corpus() for alternate analyses

corpus = corpus(dat$Title)

tokens = tokens(corpus, remove_punct = TRUE, remove_numbers = TRUE) %>% 
  tokens_remove(stopwords("english")) %>% 
  tokens_remove("http[^[:space:]]*", valuetype = "regex") %>% 
  tokens_tolower()

tokens_at = tokens(corpus, remove_punct = TRUE, remove_numbers = TRUE) %>% 
  tokens_keep(pattern = "@*")

dfm_at = dfm(tokens_at)

#this is useful!
tstat_freq <- textstat_frequency(dfm_at, n = 100)

tidy_at <- tidy(dfm_at) %>%
   count(term) %>%
   with(wordcloud(term, n, max.words = 25))

# Show the top 10 accounts
topten = data.frame(tstat_freq) %>% 
  filter(rank < 11) %>%
  select(-group) %>% 
  datatable()
topten
```

# Alternate Sentiment Comparison

```{r}
raw_sent <- raw_tweets[,c(5,7,11)] # Extract Date and Title fields

tweetsent <- tibble(text = raw_sent$Title,
                  id = seq(1:length(raw_sent$Title)),
                 date = as.Date(raw_sent$Date,'%m/%d/%y'),
                 sent = raw_sent$Sentiment)


```

Side Quest: Neutral Comparison

```{r}
tweets_sent <- clean_twt %>%
  left_join(
    words %>%
      mutate(sent_score = replace_na(sent_score, 0)) %>% 
      group_by(id) %>%
      summarize(
        sent_score = mean(sent_score, na.rm = T)),
    by = "id")

neutral <- length(which(tweets_sent$sent_score == 0))
positive <- length(which(tweets_sent$sent_score > 0))
negative <- length(which(tweets_sent$sent_score < 0))

# graph overall sentiments
Sentiment <- c("Positive","Neutral","Negative")
Count <- c(positive,neutral,negative)
output <- data.frame(Sentiment,Count)
output$Sentiment<-factor(output$Sentiment,levels=Sentiment)
ggplot(output, aes(x=Sentiment,y=Count))+
  geom_bar(stat = "identity", aes(fill = Sentiment))+
  scale_fill_manual("legend", values = c("Positive" = "green", "Neutral" = "black", "Negative" = "red"))+
  ggtitle("Barplot of Sentiment in IPCC tweets")
```

